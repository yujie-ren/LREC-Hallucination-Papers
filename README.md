# LREC-Hallucination-Papers

### LREC-COLING-2024
[[All Papers]](https://aclanthology.org/events/lrec-2024/)  
1. A Cause-Effect Look at Alleviating Hallucination of Knowledge-grounded Dialogue Generation [[pdf]](https://aclanthology.org/2024.lrec-main.9/)  
(THU, propose a solution for alleviating the hallucination in KGD, 8 pages)  
2. Benchmarking Hallucination in Large Language Models Based on Unanswerable Math Word Problem [[pdf]](https://aclanthology.org/2024.lrec-main.196/)  
(ECNU, Dataset of 5200 questions, Math problem, 5 pages)  
3. Detecting Hallucination and Coverage Errors in Retrieval Augmented Generation for Controversial Topics [[pdf]](https://aclanthology.org/2024.lrec-main.423/)  
(Google, Detect hallucination in RAG, 8 pages)  
4. Detection, Diagnosis, and Explanation: A Benchmark for  Chinese Medial Hallucination Evaluation [[pdf]](https://aclanthology.org/2024.lrec-main.428/)  
(PKU, Hallucination detection benchmark, Chinese medical evaluation, 8.5 pages)  
5. German Also Hallucinates! Inconsistency Detection in News Summaries with the Absinth Dataset [[pdf]](https://aclanthology.org/2024.lrec-main.680/)  
(ETH, Hallucination detection dataset, German news summarization, 4.5 pages)  
6. Halwasa: Quantify and Analyze Hallucinations in Large Language Models:  Arabic as a Case Study [[pdf]](https://aclanthology.org/2024.lrec-main.705/)  
(Qatar, Arabic dataset, 10k LLM-generated and annotated data, 6.5 pages)
<!--stackedit_data:
eyJoaXN0b3J5IjpbMTE3ODgwMDM0NiwtMTI3MjAyNjI3NSwtMT
I0ODYyNDA4NSwtMTY2NTg1NzE4OSw3OTc1MzI4NDcsLTE4MzUy
MjQ3OTksLTE0NDkyMzU4MTAsLTIwNjcxMzQ5N119
-->
